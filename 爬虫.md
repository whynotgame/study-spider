爬虫

​    目的：获取网页数据



​    1.获取一个网页数据用

​    urllib.request.urlopen()    #括号内是网页url

​    获取网页数据

​    补充:urlopen是使用get方法请求网页的



​    2.我们把获取的东西赋值给response    [*这个response可以是任何，反正只是传值，比如p=urllib.request.urlopen()也可以*]

​    response=urllib.request.urlopen()

​    把网页数据给了response



​    3.输出这个数据用

​    print(response.read().decode('utf-8'))    [*read()方法不知道是怎么来的，但不理他这样就能读*;*为了让这个数据编码输出是中国字，在后面多加一个方法 decode('utf-8')*]

​    输出网页数据

   

完整代码    [获取百度的网页代码]

![image-20230411165134635](C:\Users\xiaoming\AppData\Roaming\Typora\typora-user-images\image-20230411165134635.png)





2023.4.14



​    学习自己创建hadler





​    问题1：有报错 问题不知

urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:992)>



   解决问题： 关闭ssl认证

```
import ssl
ssl._create_default_https_context = ssl._create_unverified_context
```



保存cookie

使用cookie

  1.先创建一个cookie

  2.创建一个handler

  3.创建一个opener

  4.使用这个opener

  结果有可能是数组，也可能直接读出



判断error类型

 1.urlerror 好像是万能的

 2.httperror是urlerror的子类



解析url内容

使用urlparse（不能直接调用，得使用urllib.parse import出来）

scheme是协议，netloc是域名，path是路径，params是参数，query是查询条件，fragment是锚点

​                                     前/                 /后                 ;后                         ?后                            #后



构造url内容

使用数据结构长度必须为6（只能为6，但是内容可以为空），数据结构可以是元组(*)，列表[*]，

集合不行，好像不能获取。字典可以，但只获取了key

为什么要构造url，这里的方法不是连接两个或者多个字符串吗？？



urljoin更好，它可对缺失的部分自动补充，补充的内容与第一个参数没啥关系

1.使用时要注意参数都要使用https协议，要么一个用一个不用，要么两个都是https协议

2.它是根据第二个参数进行修改的，第二个参数修改的一句是按照第一个参数来的，缺什么补什么，都有就使用第二个

3.我只能说没有加号好用



url更像是一个字符串，可以随意加进去



urlencode可以把字典转为带有**‘&‘**符号的字符串

```
params = {
    'name':'jack',
    'age':16
}
```

转为  name=jack&age=16 

内涵<class 'dict'>转为<class 'str'>



parse_qs

把name=jack&age=16 转为

params = {
    'name':'jack',
    'age':16
}

**字典使用 params['name']**



parse_qsl

把name=jack&age=16 转为

[('name', 'jack'), ('age', '16')]



在url中加点中文可能导致问题

使用quote转为url认识的其他字符



在url中发现一些奇怪的玩意

使用unquote或许能还原



robots

为什么可以同时存在两个user-agent和disallow，应当相信哪一 个

解决方法，每一个爬取的url都经过  urllib的robotparser的RobotFileParser的can_fetch，去验证能否爬取

 方法: 先得到爬取网站的robots协议，再解析这个robots协议(使用parse)，再判断能否爬取网站(使用can_fetch)



**requests库**

这个东西比上面好用多了

1.requests.get方法加入参数params，不用在url后面加问号(?)

问题:不知道怎么阅读网页的方法



爬取出现问题

requests.exceptions.SSLError: HTTPSConnectionPool(host='ssr1.scrape.center', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:992)')))

说是SSL问题

使用requests库方法的get方法

解决，使用get方法里的

```
verify=False
```

后还有一点问题

warnings.warn(

解决方法:

```
import urllib3

urllib3.disable_warnings()
```

完成，不过那个好像是自欺欺人的方式，但是看起来好



爬取图片

爬取图片只能获取其二进制数据，要想看到图片要把二进制数据读取

使用with处理文件对象(说更好，但我没觉得)

```
with open('favicon.ico','wb') as f:
    f.write(r.content)
    
    2.解法
    ***=open()
    ***.wirte()
```

as 只能在import with except后面使用

f.rwite是使用对象方法    r.content是数据内容

wb表示二进制写，不存在就创建，存在就覆盖(参考:https://www.runoob.com/python/python-func-open.html)

读取ico文件再读出来好像只能以ico文件结尾读出



session

两次get网页，最新的会覆盖掉之前的一个，就像是又打开了一个网页

session使用需要先创建一个session对象    s = requests.Session()，然后两次get发现，没有相同后缀的，会补上，即两个信息一同出现。有相同后缀的，会后一个覆盖前一个

覆盖:

```
s.get('https://www.httpbin.org/cookies/set/number/112233')
r = s.get('https://www.httpbin.org/cookies/set/number/123456789')
```

补上:

```
s.get('https://www.httpbin.org/cookies/set/name/112233')
r = s.get('https://www.httpbin.org/cookies/set/number/123456789')
```

感觉上就是，点开了网页内一个网页，它保存了你之前网页留下的信息



ssl（验证）

文件命名不能为ssl.py （不知道为什么）

get内部可以加一个vertify 含义是不要去验证证书了，但是它会再让你给它指定证书，看起来像报错

```
from requests.packages import urllib3

urllib3.disable_warnings()
```

加入以上代码，就可以忽略这个警告了

还可以捕获日志，指定文件作为客户端证书（捕获日志不知道在哪看。没有文件是证书）



timeout

在get方法中，可以设置timeout，含义是超时我就报错。时间包括两个阶段,连接和读取（根本不能分辨这两个阶段）,不加这个参数，默认是永久等待（这样就会卡死在这个步子上，建议加timeout）



身份认证

requests.get可以直接设置身份认证所需的内容，在后面加auth(auth翻译认证)

```
r = requests.get('https://ssr3.scrape.center/',auth=('admin','admin'))
```

requests还有其他认证方式，比如OAuth1,HTTPBasicAuth，但是看不懂有什么区别,OAuth好像有点东西藏着

OAuth 不是一个API或者服务，而是一个验证授权(Authorization)的开放标准（参考网站[OAuth2.0 详解 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/89020647)）



proxies(代理) 

proxy也是代理

我猜可能是不要用一个地址频繁的请求一个网站，为了这个目的出现的代理

在网上查到一个代理网站([🤖 Free Proxy List [1,184 IPs Online\] (geonode.com)](https://geonode.com/free-proxy-list))好像只能用socks进行代理

使用之后报错（由于目标计算机积极拒绝，无法连接），网上一般是自己建网站然后代理，所以能解决。但是我用网上给的代理网站进行登录，就是用不了

还有，代理的意义不明确，即不知道他的流通过程，不知道哪一步有问题 解决([python构建IP代理池（Proxy Pool）[通俗易懂\]-腾讯云开发者社区-腾讯云 (tencent.com)](https://cloud.tencent.com/developer/article/2087761))

我发给代理，代理给目标，目标给代理，代理给我

成功，就是那个破网站不行，或者socks5不会用

https可以不填，也可以实现代理



自己构造requests方法（prepare：准备）

首先引入一个Requests对象（from requests import Request），放入对应数据（'POST',url,data=...,headers=...）,再调用Session中的prepare_requests方法转换为一个对象（根本不知道为什么转换）,然后使用session中的send方法把这个转换后的对象发送过去，就可以接受到信息了

1.为什么一定要使用session

2.构造对象放入数据我可以理解，为什么要把它转换为一个不知道干啥用的对象(即不知道session中的prepare_requests到底干啥)

构造requests方法思路:用session.send方法把数据发到指定网站，数据的格式要求要用prepare_requests转换，数据的内容由自己定义。（与get，post方法一样，可以写data,headers在里）



正则表达式测试

文本：Hello,my phone number is 010-4165456165 and email is wocai@445566.com, and my website is https://www.woshishei.com

方法：使用re.match(A,B)，第一个内容写正则表达式，第二个内容写需要正则表达式分解的内容

输出结果是用match之后的结果.group()一下得到的

在正则表达式内，可以将想要的结果使用（）括起来。然后在输出时，在.group()的括号内加入1，2...来获取所需要的数据



通用匹配

正则表达式内加入.*表示任意全部匹配，可以直接全部匹配

^H表示以H开头的，结果只匹配一个字符"H"

m$表示以m结尾的，但是不能只单单使用这个匹配，不知道为什么

```
result = re.match('o$',content)
```

（上面的是错的），不知道错的原因



贪婪与非贪婪

正则表达式，.*代表全部匹配，但是会尽可能的匹配，与()使用，还会留下一个

.*?是尽可能少的匹配,与()使用，尽可能不匹配

```
result = re.match('.*?(\d+)',content)
```

上面就是尽可能在不是数字前少匹配，所以一到数字就停止

```
result = re.match('.*(\d+)',content)
```

这个是尽可能在数字前匹配，所以只留下一个数字



修饰符 (修饰正则表达式的匹配内容，比如不匹配这个，匹配那个)

在正则表达式中，首先是匹配到字符串全部（意思是如果不到最后那个‘’‘是不能用结尾字符的）。然后使用换行符，正则表达式不匹配换行字符。要想让正则表达式匹配换行字符，需要添加修饰符re.S



转义字符

re.match使用，(A,B),A是匹配内容，可以直接是B，也可以挑B内的东西，用字符替代。B是要匹配的内容

当要匹配的内容有与替代字符相同的内容，用\去转义这个字符



search方法（查到就返回）

match方法：是从头开始匹配，如果没有就返回None

search方法：是诶个查，查到第一个就返回匹配内容，全部查完没有才返回None

正则表达式使用的时候尽量表示到精确一点的位置

```
result = re.search('.*?singer=(.*?)>',content,re.S)
```

上面正则表达式表示，在content内容中找 尽量少的内容，以singer=开头的，尽量少的内容，以>结尾，忽略回车字符

```
result = re.search('.*?singer=(.*)>',content,re.S)
```

这个正则表达式表示，找尽量少的，以singer开头的，尽量多的内容，以>结尾的

这两个，结果相当不一样



findall方法（全查，内容全部分割）

查询全部符合的内容。只要中间不连续，即使是一整个字符串，也会分割

会以一个列表包含元组的方式返回[()]，这样的方式可以使用for返回值 for i in result，i的返回是一个元组，元组也可以用for遍历。

后面一定会接触json返回值，一定会有字典，暂时不去考虑



sub方法（全查，内容全部替换，替换内容严格按照正则表达式，括号内也匹配）

把正则表达式内容替换，必须要三个参数

```
result = re.sub('\d+','',content)
```

上面内容是把content内容中，带数字的，替换成空

???为什么能匹配到全部内容

答：就是可以

\n好像是一个很特殊的东西(换行)，他好像不能匹配，用.*?都不能匹配，需要之后给他去掉，方法： .strip()，默认去掉空格换行

sub是严格替换，连空格都给留下来



compile方法（和sub替换方法一起用）

compile()内写的是要用这则表达式替换的东西,sub()，的第一个可以用compile替换

```
pattern = re.compile('\d')
result = re.sub(pattern,'',content1)#严格替换，连空格都留下来
```

看起来就是一个封装，没有多大意义，看不明白



httpx（网站有个协议，http2.0用这个，其他requests库就行）

httpx默认不开启http2，需要代码开启

```
clinet = httpx.Client(http2=True)

response = clinet.get('https://spa16.scrape.center/')
```

可以从返回数值用.http_version看出来使用的协议，但是不能对response报错的使用这个,因为传回来的值本身就是错的



异步请求

根本看不懂(定义模块前面还加一个模块，不知道含义,with前面加一个包，不知道含义)

```
async def fetch(url): #根本看不懂
    async with httpx.AsyncClient(http2=True) as clinet:
        response = await clinet.get(url)
        print(response.text)

if __name__ == '__main__':
    asyncio.get_event_loop().run_until_complete(fetch('https://www.httpbin.org/get'))
```

dir(包名字),可以传回包包含的方法



demo1

分析[Scrape | Movie](https://ssr1.scrape.center/)网页

1.发现网页内容使用el-row包含一个

2.照片使用一个链接，链过去可以直接打开网页

3.网页有个detail详情界面，在原先url基础上加/detail/数字 就能访问

4.列表是使用ul li形式访问的 /page/数字 即可

爬取网页网址：

先用for，挨个得到列表的url；再进入这个列表网页，获取这个网页html，然后分离出来超链接中详情界面的url，链接到根url上。

我使用logging方式打印结果,logging需要先定义打印格式

```
logging.basicConfig(level=logging.INFO,format='%(asctime)s - %(levelname)s: %(message)s')
```

使用logging.info打印

```
logging.info('scraping %s...',url)
```

用try...expect...让程序健硕一点

try是执行程序，expect是执行程序有什么错误，会进入到他这里

```
requests.RequestException #包括好多requests的错误
```

[Python3：Requests模块的异常值处理RequestException_requestexception用法python-CSDN博客](https://blog.csdn.net/qq_40984952/article/details/105044771)

```
try:
    response = requests.get(url)
    if response.status_code == 200:
        return response.text
    else:
        logging.error('get invaild status code %s while scraping %s',response.status_code,url) 
except requests.RequestException: #所有其他类型错误
    logging.error('error occured while scraping %s',url,exc_info=True)
```

其中exc_info说是打印错误堆栈信息，我也不懂

yield:返回一个东西，和return一样

可以用next，list打印，但是不一样这两个

```
for item in items:
    detail_url = urljoin(base_url, item)
    # print(detail_url)
    logging.info('get detail url %s',detail_url)
    yield detail_url #返回一个数值
```

用next好像只打印第一个，用list打印全部。就像在for中，把返回东西全部存在一个地方，next访问第一个，list访问全部

```
logging.info('detail urls %s',list(detail_urls)) #list与next
```

***关键点：html的正则表达式获取，是根据返回来的值确定的，而不是根据网页来的***



Xpath中的etree模块

[解析库xpath高级使用（超全） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/134126265)

是把文本转为可以使用的文本，好像什么文本都可以

```
html = etree.HTML(text)
result = etree.tostring(html) #是byte类型
print(result.decode("utf-8")) #.decode是把byte转为字符
```

html把文本初始化，构造一个可以解析的对象（看不懂含义）

result用tostring把文本修正为html代码，但是是byte类型

.decode("utf-8")把byte转为字符串

我感觉就是将文本加了一个html,body标签，好像啥也没干

补了，在输出下面</li></ul>

```
html = etree.parse('./test.html',etree.HTMLParser())
```

这样解析文件.test.html，好像是构造一个解析对象，但是结果有转义字符&#13意思是回车,也不知道怎么去掉



parse 分析页面含有的内容，结果以数组返回出来

先把文件内容转为一个可以解析对象，然后在同xpath把要取的东西写出来

```
html = etree.parse('./test.html',etree.HTMLParser())
result = html.xpath('//li')
print(result[0])
```

这里是匹配所有li节点

//是匹配所有这个节点

但是不明白返回内容的含义

<Element li at 0x1882fba7480>

```
<div>
    <ul>
        <li class="item-0"><a href="link1.html">first item</a></li>
        <li class="item-1"><a href="link2.html">second item</a></li>
        <li class="item-inactive"><a href="link3.html">third item</a></li>
        <li class="item-1"><a href="link4.html">fourth item</a></li>
        <li class="item-0"><a href="link5.html">fifth item</a>
    </ul>
</div>
```

```
result = html.xpath('//li/a')
```

获取li的直接a节点，而且是全部内容。当把li改为ul，就获取不到了，是因为/获取的是子节点

```
result = html.xpath('//li//a')
```

获取li的所有子孙节点a。这时把li换为ul，就能全部获得了。